{"author":{"id":"U_kgDOBnWPLA","is_bot":false,"login":"sam-at-block","name":"Sam Osborn"},"body":"**Motivation**\nWhen configuring OpenAI compatible providers, that are serving various open source models, the model names that the server expects can be hard to track down. \nI ended up finding the right model name by running this `curl \"$URL/v1/models | jq`\n\n**Describe the solution you'd like**\nAs a QoL improvement, when configuring the LLM Provider, after providing the host/token stuff, and asking which model to use, Goose can proactively fetch all models that the API serves, and suggest them. This could probably work in both the GUI and CLI, but I was using the cli, so maybe the model prompt section could try and get a list to provide a single select prompt in the config wizard. . \n\nAt least OpenAI compatible APIs support this GET /v1/models list, but Im assuming most API formats in the wild have some sense of \"list available models\". But even the OpenAI model options are kinda confusing, lots of options, so a prepopulated list would help remove friction from the initial onboarding. ","comments":[],"createdAt":"2025-03-11T19:15:47Z","labels":[{"id":"LA_kwDOMneZ988AAAABtyYvwQ","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"number":1621,"state":"OPEN","title":"LLM Provider config wizard should list supported models from server for ease of setup","url":"https://github.com/block/goose/issues/1621"}
